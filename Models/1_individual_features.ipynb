{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a361fd",
   "metadata": {
    "id": "81a361fd"
   },
   "source": [
    "# Individual features for AI detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff777b4",
   "metadata": {},
   "source": [
    "This notebook contains the code for developing the lightweight stylometric features for my baseline model and bias related features for my bias model.\n",
    "\n",
    "With this code, each feature can be run on training data to detect if they seem worthwhile and how AI vs. human texts perfom with each feature. However, it requires a local copy of the PAN Task data from: https://zenodo.org/records/14962653\n",
    "\n",
    "Of the features developed here, the following features were not moved into features.py for later use:\n",
    "- Baseline: TF-IDF\n",
    "- Bias: Emoji rate\n",
    "- Bias: Politeness\n",
    "- Bias: Political leaning\n",
    "- Bias: We vs. They"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685c3ab",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be788aff",
   "metadata": {
    "id": "be788aff"
   },
   "outputs": [],
   "source": [
    "# --- Set up environment ---\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95cfb9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "training_data = #Insert path to your training data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617b5f88",
   "metadata": {
    "id": "617b5f88"
   },
   "outputs": [],
   "source": [
    "# Keep original and metadata\n",
    "train_path = Path(training_data)\n",
    "df_train = pd.read_json(train_path, lines=True)\n",
    "df_train = df_train[[\"id\", \"text\", \"label\", \"genre\", \"model\"]].copy()\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ae5125",
   "metadata": {
    "id": "c8ae5125"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [32:43<00:00, 12.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Apply spaCy to get all tokens, POS tags, etc.\n",
    "df_train[\"spacy_doc\"] = df_train[\"text\"].progress_apply(nlp) # takes 40 to 90 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff228370",
   "metadata": {
    "id": "ff228370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char TF-IDF shape: (23707, 50000)\n",
      "[' \"a' ' \"a ' ' \"ah' ' \"ah,' ' \"al' ' \"an' ' \"and' ' \"b' ' \"be' ' \"bu'\n",
      " ' \"but' ' \"c' ' \"ca' ' \"co' ' \"com' ' \"d' ' \"di' ' \"do' ' \"do ' ' \"e']\n"
     ]
    }
   ],
   "source": [
    "# --- Char-level TF-IDF vectorizer ---\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    lowercase=True,\n",
    "    max_features=50000)\n",
    "\n",
    "# Apply to raw text\n",
    "X_char = char_vectorizer.fit_transform(df_train[\"text\"])\n",
    "\n",
    "# Check output\n",
    "print(f\"Char TF-IDF shape: {X_char.shape}\") # (num_texts, num_features)\n",
    "print(char_vectorizer.get_feature_names_out()[:20]) # first 20 results (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb492b9",
   "metadata": {
    "id": "deb492b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF shape: (23707, 50000)\n",
      "['a' 'a a' 'a baby' 'a bachelor' 'a back' 'a backdrop' 'a bad' 'a balance'\n",
      " 'a balanced' 'a ball' 'a balm' 'a band' 'a bank' 'a bargain' 'a barrier'\n",
      " 'a basket' 'a bastion' 'a battle' 'a battleground' 'a beacon']\n"
     ]
    }
   ],
   "source": [
    "# --- Word-level TF-IDF vectorizer ---\n",
    "# Remove numbers from text before vectorizing\n",
    "df_train[\"text_no_numbers\"] = df_train[\"text\"].str.replace(r\"\\d+\", \"\", regex=True)\n",
    "\n",
    "# Word-level TF-IDF vectorizer without numbers\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "# Apply to raw text without numbers\n",
    "X_word = word_vectorizer.fit_transform(df_train[\"text_no_numbers\"])\n",
    "\n",
    "# Check output\n",
    "print(f\"Word TF-IDF shape: {X_word.shape}\") # (num_texts, num_features)\n",
    "print(word_vectorizer.get_feature_names_out()[:20]) # first 20 results (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10c8d9",
   "metadata": {
    "id": "3b10c8d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:34<00:00, 694.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean_sent_len  std_sent_len  mean_word_len  std_word_len\n",
      "label                                                          \n",
      "0          23.492785     14.494070       4.326734      2.316890\n",
      "1          23.612674      8.521213       4.998295      2.716548\n",
      "       mean_sent_len  std_sent_len  mean_word_len  std_word_len\n",
      "label                                                          \n",
      "0           7.974506      6.180024       0.337722      0.249901\n",
      "1           7.900751      6.590442       0.654281      0.570322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Sentence and word length ---\n",
    "def extract_length_features(doc):\n",
    "    # Sentences (filtered for valid tokens)\n",
    "    sent_lengths = [\n",
    "        len([t for t in sent if not t.is_punct])\n",
    "        for sent in doc.sents\n",
    "        if len(sent) > 0\n",
    "    ]\n",
    "\n",
    "    # Words (exclude punctuation)\n",
    "    words = [t for t in doc if not t.is_punct]\n",
    "    word_lengths = [len(t.text) for t in words]\n",
    "\n",
    "    # Compute stats\n",
    "    return pd.Series({\n",
    "        \"mean_sent_len\": np.nanmean(sent_lengths) if sent_lengths else 0,\n",
    "        \"std_sent_len\":  np.nanstd(sent_lengths) if len(sent_lengths) > 1 else 0,\n",
    "        \"mean_word_len\": np.nanmean(word_lengths) if word_lengths else 0,\n",
    "        \"std_word_len\":  np.nanstd(word_lengths) if len(word_lengths) > 1 else 0,\n",
    "    })\n",
    "\n",
    "# Apply to spaCy text\n",
    "length_features = df_train[\"spacy_doc\"].progress_apply(extract_length_features)\n",
    "\n",
    "# Combine features with labels\n",
    "length_features_labeled = length_features.copy()\n",
    "length_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(length_features_labeled.groupby(\"label\").mean())\n",
    "print(length_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b61bbd",
   "metadata": {
    "id": "83b61bbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:09<00:00, 2594.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stopword_ratio\n",
      "label                \n",
      "0            0.559110\n",
      "1            0.467336\n",
      "       stopword_ratio\n",
      "label                \n",
      "0            0.053475\n",
      "1            0.059773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Stop words ---\n",
    "def extract_stopword_rate(doc):\n",
    "    tokens = [t for t in doc if not t.is_punct]\n",
    "    num_tokens = len(tokens)\n",
    "    num_stopwords = sum(t.is_stop for t in tokens)\n",
    "\n",
    "    stopword_ratio = num_stopwords / num_tokens if num_tokens > 0 else 0\n",
    "    return pd.Series({\n",
    "        \"stopword_ratio\": stopword_ratio\n",
    "    })\n",
    "\n",
    "# Apply to spaCy text\n",
    "stopword_features = df_train[\"spacy_doc\"].progress_apply(extract_stopword_rate)\n",
    "\n",
    "# Combine features with labels\n",
    "stopword_features_labeled = stopword_features.copy()\n",
    "stopword_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(stopword_features_labeled.groupby(\"label\").mean())\n",
    "print(stopword_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ca31c",
   "metadata": {
    "id": "240ca31c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:04<00:00, 5414.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         punct_,   punct_.   punct_;   punct_:   punct_?   punct_!   punct_'  \\\n",
      "label                                                                          \n",
      "0      13.406649  8.800357  1.192362  0.255844  0.788814  0.673093  2.168841   \n",
      "1      12.626805  7.935055  0.132706  0.127867  0.360970  0.141684  2.033503   \n",
      "\n",
      "        punct_\"   punct_(   punct_)   punct_-   punct_‚Ä¶  \n",
      "label                                                    \n",
      "0      3.173604  0.144225  0.144691  2.926624  0.009305  \n",
      "1      2.155658  0.097837  0.097856  0.960964  0.033617  \n",
      "        punct_,   punct_.   punct_;   punct_:   punct_?   punct_!   punct_'  \\\n",
      "label                                                                         \n",
      "0      3.757094  3.114569  1.100631  0.423927  0.948124  0.961134  3.611919   \n",
      "1      4.225618  2.766029  0.294887  0.346481  0.673576  0.463970  2.313164   \n",
      "\n",
      "        punct_\"   punct_(   punct_)   punct_-   punct_‚Ä¶  \n",
      "label                                                    \n",
      "0      5.021078  0.387232  0.388396  2.756644  0.080972  \n",
      "1      3.117307  0.328473  0.327992  1.063832  0.214208  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Punctuation ---\n",
    "# Define punctuation marks\n",
    "punct_marks = [\",\", \".\", \";\", \":\", \"?\", \"!\", \"'\", '\"', \"(\", \")\", \"-\", \"‚Ä¶\"]\n",
    "\n",
    "def extract_punct_rates(text):\n",
    "    total_chars = len(text)\n",
    "    counts = {f\"punct_{p}\": text.count(p) / total_chars * 1000 if total_chars > 0 else 0\n",
    "              for p in punct_marks}\n",
    "    return pd.Series(counts)\n",
    "\n",
    "# Apply to raw text\n",
    "punct_features = df_train[\"text\"].progress_apply(extract_punct_rates)\n",
    "\n",
    "# Combine features with labels\n",
    "punct_features_labeled = punct_features.copy()\n",
    "punct_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(punct_features_labeled.groupby(\"label\").mean())\n",
    "print(punct_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88e6f8",
   "metadata": {
    "id": "fa88e6f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:20<00:00, 1148.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       upper_ratio  title_ratio  digit_ratio  space_ratio\n",
      "label                                                    \n",
      "0         0.021202     0.103604     0.001172     0.179693\n",
      "1         0.018649     0.098261     0.001543     0.161593\n",
      "       upper_ratio  title_ratio  digit_ratio  space_ratio\n",
      "label                                                    \n",
      "0         0.009640     0.037458     0.003460     0.009878\n",
      "1         0.009516     0.043080     0.003507     0.013396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Casing ratios ---\n",
    "def extract_char_class_ratios(text):\n",
    "    total_chars = len(text)\n",
    "    total_tokens = len(text.split())\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if total_chars == 0:\n",
    "        return pd.Series({k: 0 for k in [\"upper_ratio\", \"title_ratio\", \"digit_ratio\", \"space_ratio\"]})\n",
    "\n",
    "    upper_ratio = sum(c.isupper() for c in text) / total_chars\n",
    "    digit_ratio = sum(c.isdigit() for c in text) / total_chars\n",
    "    space_ratio = sum(c.isspace() for c in text) / total_chars\n",
    "\n",
    "    # Titlecase = first letter uppercase, rest lowercase\n",
    "    title_ratio = sum(w.istitle() for w in text.split()) / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    return pd.Series({\n",
    "        \"upper_ratio\": upper_ratio,\n",
    "        \"title_ratio\": title_ratio,\n",
    "        \"digit_ratio\": digit_ratio,\n",
    "        \"space_ratio\": space_ratio\n",
    "    })\n",
    "\n",
    "# Apply to raw text\n",
    "casing_features = df_train[\"text\"].progress_apply(extract_char_class_ratios)\n",
    "\n",
    "# Combine features with labels\n",
    "casing_features_labeled = casing_features.copy()\n",
    "casing_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(casing_features_labeled.groupby(\"label\").mean())\n",
    "print(casing_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ae2f8",
   "metadata": {
    "id": "ea9ae2f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:14<00:00, 1623.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ttr\n",
      "label          \n",
      "0      0.473008\n",
      "1      0.526680\n",
      "            ttr\n",
      "label          \n",
      "0      0.047294\n",
      "1      0.065601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Type‚ÄìToken ratio ---\n",
    "def extract_ttr(doc):\n",
    "    tokens = [t.text.lower() for t in doc if t.is_alpha]\n",
    "    num_tokens = len(tokens)\n",
    "    num_types = len(set(tokens))\n",
    "\n",
    "    ttr = num_types / num_tokens if num_tokens > 0 else 0\n",
    "    return pd.Series({\"ttr\": ttr})\n",
    "\n",
    "# Apply to spaCy text\n",
    "ttr_feature = df_train[\"spacy_doc\"].progress_apply(extract_ttr)\n",
    "\n",
    "# Combine features with labels\n",
    "ttr_features_labeled = ttr_feature.copy()\n",
    "ttr_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(ttr_features_labeled.groupby(\"label\").mean())\n",
    "print(ttr_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f30a2",
   "metadata": {
    "id": "014f30a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:11<00:00, 2093.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       upos_NOUN  upos_VERB  upos_ADJ  upos_ADV  upos_PRON  upos_ADP  \\\n",
      "label                                                                  \n",
      "0       0.181809   0.128340  0.072362  0.053546   0.121642  0.115584   \n",
      "1       0.242442   0.128684  0.087093  0.038426   0.082271  0.125813   \n",
      "\n",
      "       upos_DET  upos_CCONJ  upos_SCONJ  upos_NUM  upos_AUX  upos_INTJ  \\\n",
      "label                                                                    \n",
      "0      0.097427    0.040256    0.025845  0.009726  0.065546   0.002768   \n",
      "1      0.111099    0.032557    0.019728  0.005655  0.037501   0.000715   \n",
      "\n",
      "       upos_PART  upos_PROPN  \n",
      "label                         \n",
      "0       0.029937    0.054544  \n",
      "1       0.028116    0.059417  \n",
      "       upos_NOUN  upos_VERB  upos_ADJ  upos_ADV  upos_PRON  upos_ADP  \\\n",
      "label                                                                  \n",
      "0       0.035080   0.019883  0.018824  0.014838   0.041499  0.019767   \n",
      "1       0.036582   0.018593  0.027313  0.015071   0.042357  0.019193   \n",
      "\n",
      "       upos_DET  upos_CCONJ  upos_SCONJ  upos_NUM  upos_AUX  upos_INTJ  \\\n",
      "label                                                                    \n",
      "0      0.021138    0.011393    0.008735  0.009170  0.017575   0.003732   \n",
      "1      0.024107    0.011087    0.008388  0.008431  0.016646   0.001662   \n",
      "\n",
      "       upos_PART  upos_PROPN  \n",
      "label                         \n",
      "0       0.009967    0.036257  \n",
      "1       0.011188    0.045347  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UPOS distribution --\n",
    "# Fixed list of tags to extract\n",
    "upos_tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'ADP', 'DET', 'CCONJ', 'SCONJ', 'NUM', 'AUX', 'INTJ', 'PART', 'PROPN']\n",
    "\n",
    "def extract_upos_freq(doc):\n",
    "    tags = [t.pos_ for t in doc if not t.is_punct and not t.is_space]\n",
    "    total = len(tags)\n",
    "\n",
    "    tag_counts = Counter(tags)\n",
    "    freqs = {\n",
    "        f\"upos_{tag}\": tag_counts.get(tag, 0) / total if total > 0 else 0\n",
    "        for tag in upos_tags\n",
    "    }\n",
    "    return pd.Series(freqs)\n",
    "\n",
    "# Apply to spaCy text\n",
    "upos_features = df_train[\"spacy_doc\"].progress_apply(extract_upos_freq)\n",
    "\n",
    "# Combine features with labels\n",
    "upos_features_labeled = upos_features.copy()\n",
    "upos_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(upos_features_labeled.groupby(\"label\").mean())\n",
    "print(upos_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a16ef",
   "metadata": {
    "id": "cf1a16ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:26<00:00, 901.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       rep_5gram_ratio\n",
      "label                 \n",
      "0             0.022223\n",
      "1             0.032773\n",
      "       rep_5gram_ratio\n",
      "label                 \n",
      "0             0.038228\n",
      "1             0.068343\n"
     ]
    }
   ],
   "source": [
    "# --- Max 5-gram repetition rate ---\n",
    "def compute_5gram_repetition(doc):\n",
    "    # Use alpha-only tokens (optional: can include all if you prefer)\n",
    "    tokens = [t.text.lower() for t in doc if not t.is_space]\n",
    "    n = 5\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    if total_tokens < n:\n",
    "        return pd.Series({\"rep_5gram_ratio\": 0.0})\n",
    "\n",
    "    # Count all 5-grams\n",
    "    counts = defaultdict(int)\n",
    "    for i in range(total_tokens - n + 1):\n",
    "        fivegram = tuple(tokens[i:i+n])\n",
    "        counts[fivegram] += 1\n",
    "\n",
    "    # Find all repeated 5-grams\n",
    "    repeated_spans = set()\n",
    "    for i in range(total_tokens - n + 1):\n",
    "        fivegram = tuple(tokens[i:i+n])\n",
    "        if counts[fivegram] > 1:\n",
    "            repeated_spans.update(range(i, i+n))\n",
    "\n",
    "    # Compute repetition ratio\n",
    "    rep_ratio = len(repeated_spans) / total_tokens if total_tokens > 0 else 0\n",
    "    return pd.Series({\"rep_5gram_ratio\": rep_ratio})\n",
    "\n",
    "# Apply to spaCy text\n",
    "rep_features = df_train[\"spacy_doc\"].progress_apply(compute_5gram_repetition)\n",
    "df_train[\"token_count\"] = df_train[\"spacy_doc\"].apply(lambda doc: len([t for t in doc if not t.is_space]))\n",
    "\n",
    "# Combine features with labels\n",
    "rep_features_labeled = rep_features.copy()\n",
    "rep_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(rep_features_labeled.groupby(\"label\").mean())\n",
    "print(rep_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb62d5",
   "metadata": {
    "id": "1cdb62d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:17<00:00, 1339.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       self_sim_jaccard\n",
      "label                  \n",
      "0              0.180080\n",
      "1              0.167926\n",
      "       self_sim_jaccard\n",
      "label                  \n",
      "0              0.038875\n",
      "1              0.047826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Self-similarity ---\n",
    "def compute_self_similarity(doc, k=200):\n",
    "    # Use clean lowercase tokens\n",
    "    tokens = [t.text.lower() for t in doc if t.is_alpha]\n",
    "    total = len(tokens)\n",
    "\n",
    "    half = total // 2\n",
    "    first_half = tokens[:half]\n",
    "    second_half = tokens[half:]\n",
    "\n",
    "    # Count top-k unigrams in each half\n",
    "    first_top = set([w for w, _ in Counter(first_half).most_common(k)])\n",
    "    second_top = set([w for w, _ in Counter(second_half).most_common(k)])\n",
    "\n",
    "    # Compute Jaccard similarity\n",
    "    intersection = first_top & second_top\n",
    "    union = first_top | second_top\n",
    "\n",
    "    sim = len(intersection) / len(union) if union else 0.0\n",
    "    return pd.Series({\"self_sim_jaccard\": sim})\n",
    "\n",
    "# Apply to spaCy docs\n",
    "selfsim_features = df_train[\"spacy_doc\"].progress_apply(compute_self_similarity)\n",
    "\n",
    "# Combine features with labels\n",
    "selfsim_features_labeled = selfsim_features.copy()\n",
    "selfsim_features_labeled[\"label\"] = df_train[\"label\"] # 0=Human, 1=AI\n",
    "\n",
    "# Look at class-wise means\n",
    "print(selfsim_features_labeled.groupby(\"label\").mean())\n",
    "print(selfsim_features_labeled.groupby(\"label\").std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4fd7d",
   "metadata": {},
   "source": [
    "## Bias model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f333f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up environment ---\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pathlib import Path\n",
    "from torch import argmax\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8262482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 21.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 68.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 30.8 MB/s  0:00:00\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "training_data = #Insert path to your training data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00eab777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original and metadata\n",
    "train_path = Path(training_data)\n",
    "df_bias = pd.read_json(train_path, lines=True)\n",
    "df_bias = df_bias[[\"id\", \"text\", \"label\", \"genre\", \"model\"]].copy()\n",
    "df_bias[\"label\"] = df_bias[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [02:48<00:00, 140.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">sentiment_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">sentiment_var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089463</td>\n",
       "      <td>0.022290</td>\n",
       "      <td>0.165208</td>\n",
       "      <td>0.003891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126481</td>\n",
       "      <td>0.048832</td>\n",
       "      <td>0.167544</td>\n",
       "      <td>0.004575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment_mean           sentiment_var          \n",
       "                mean       var          mean       var\n",
       "label                                                 \n",
       "0           0.089463  0.022290      0.165208  0.003891\n",
       "1           0.126481  0.048832      0.167544  0.004575"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Sentiment analysis ---\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "def compute_sentiment_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return pd.Series({\"sentiment_mean\": 0.0, \"sentiment_var\": 0.0})\n",
    "\n",
    "    scores = [vader.polarity_scores(s)[\"compound\"] for s in sentences]\n",
    "    return pd.Series({\n",
    "        \"sentiment_mean\": np.mean(scores),\n",
    "        \"sentiment_var\": np.var(scores)\n",
    "    })\n",
    "\n",
    "# Apply to dataframe\n",
    "sentiment_features = df_bias[\"text\"].progress_apply(compute_sentiment_features)\n",
    "\n",
    "# Add results into df\n",
    "df_bias[\"sentiment_mean\"] = sentiment_features[\"sentiment_mean\"]\n",
    "df_bias[\"sentiment_var\"] = sentiment_features[\"sentiment_var\"]\n",
    "\n",
    "# Check output\n",
    "df_bias.groupby(\"label\")[[\"sentiment_mean\", \"sentiment_var\"]].agg([\"mean\", \"var\"]) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01354e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23707 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:00<00:00, 84958.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.492696e-08</td>\n",
       "      <td>5.654936e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.231006e-08</td>\n",
       "      <td>1.524779e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean           var\n",
       "label                            \n",
       "0      2.492696e-08  5.654936e-12\n",
       "1      3.231006e-08  1.524779e-11"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Emoji rate ---\n",
    "emojis = {\"üòÇ\", \"ü§£\", \"üòí\", \"üôÑ\", \"üò¨\", \"üòç\", \"üò≠\", \"üòä\", \"üò°\", \"üëç\", \":)\", \":-)\", \":(\", \":-(\"}\n",
    "\n",
    "def compute_emoji_rate(text):\n",
    "    total_chars = len(text)\n",
    "    if total_chars == 0:\n",
    "        return 0.0\n",
    "    emoji_count = sum(text.count(e) for e in emojis)\n",
    "    return emoji_count / total_chars\n",
    "\n",
    "# Apply to dataframe\n",
    "emoji_rate = df_bias[\"emoji_rate\"] = df_bias[\"text\"].progress_apply(compute_emoji_rate)\n",
    "\n",
    "# Check output\n",
    "df_bias.groupby(\"label\")[\"emoji_rate\"].agg([\"mean\", \"var\"]) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23707 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:07<00:00, 3178.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         count      mean       std       max\n",
      "label                                       \n",
      "0       9101.0  0.004868  0.004586  0.068000\n",
      "1      14606.0  0.003404  0.005084  0.091716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Profanity and insults ---\n",
    "# Load profanity / insult lexicon (one word per line)\n",
    "path = #Insert path to toxic-words-dictionary.txt here (can be downloaded from github repository)\n",
    "\n",
    "def load_profanity_lexicon(path):\n",
    "    with open(path, \"r\", encoding=\"cp1252\") as f:\n",
    "        return {line.strip().lower() for line in f if line.strip()}\n",
    "\n",
    "profanity_lexicon = load_profanity_lexicon(path)\n",
    "\n",
    "def compute_profanity_rate(text):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    count = sum(token in profanity_lexicon for token in tokens)\n",
    "    return count / len(tokens)\n",
    "\n",
    "# Apply to dataframe\n",
    "profanity_rate = df_bias[\"profanity_rate\"] = df_bias[\"text\"].progress_apply(compute_profanity_rate)\n",
    "\n",
    "# Check output\n",
    "stats_by_label = df_bias.groupby(\"label\")[\"profanity_rate\"].describe()\n",
    "stats_by_label = stats_by_label[[\"count\", \"mean\", \"std\", \"max\"]]\n",
    "print(stats_by_label) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:18<00:00, 1284.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         count      mean       std       max\n",
      "label                                       \n",
      "0       9101.0  0.002168  0.003547  0.023288\n",
      "1      14606.0  0.001213  0.002822  0.029762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Politeness and impoliteness (few results found in PAN Task data) --\n",
    "# Define lexicons\n",
    "politeness_lexicon = {\n",
    "    \"polite_markers\": [\n",
    "        # Gratitude / appreciation\n",
    "        \"please\", \"thank\", \"thanks\", \"thank you\", \"appreciate\", \"grateful\", \"many thanks\",\n",
    "        \"much obliged\", \"i am grateful\", \"i appreciate\", \"that would be great\",\n",
    "        \"thankfully\", \"cheers\",\n",
    "\n",
    "        # Apologies / softeners\n",
    "        \"sorry\", \"excuse me\", \"pardon\", \"forgive me\", \"i apologize\", \"my apologies\",\n",
    "        \"hope you don‚Äôt mind\", \"if you don‚Äôt mind\", \"if you please\", \"beg your pardon\",\n",
    "\n",
    "        # Indirect / hedging requests\n",
    "        \"if you could\", \"would you mind\", \"may i\", \"could you\", \"would you\", \"might you\",\n",
    "        \"it would be great if\", \"i was wondering if\", \"i would appreciate it if\",\n",
    "        \"do you think you could\", \"perhaps\", \"possibly\", \"hopefully\", \"maybe\", \"might be\",\n",
    "        \"i think\", \"i suppose\", \"it seems\", \"i believe\", \"in my opinion\", \"from my point of view\",\n",
    "\n",
    "        # Positive politeness / friendliness\n",
    "        \"nice\", \"kind\", \"wonderful\", \"lovely\", \"dear\", \"friendly\", \"helpful\",\n",
    "        \"great job\", \"well done\", \"good work\", \"amazing\", \"fantastic\", \"brilliant\",\n",
    "        \"pleased\", \"glad\", \"delighted\", \"it‚Äôs a pleasure\", \"so kind of you\",\n",
    "        \"how are you\", \"hope you‚Äôre well\", \"thank you so much\"\n",
    "    ],\n",
    "\n",
    "    \"impolite_markers\": [\n",
    "        # Direct insults / name-calling\n",
    "        \"stupid\", \"idiot\", \"moron\", \"dumb\", \"fool\", \"loser\", \"jerk\", \"bastard\", \"trash\",\n",
    "        \"worthless\", \"lazy\", \"ignorant\", \"pathetic\", \"useless\", \"annoying\", \"nonsense\",\n",
    "\n",
    "        # Aggressive imperatives / dismissals\n",
    "        \"shut up\", \"shut it\", \"shut your mouth\", \"get lost\", \"go away\", \"leave me alone\",\n",
    "        \"mind your own business\", \"don‚Äôt bother\", \"don‚Äôt you dare\", \"stop talking\",\n",
    "        \"listen\", \"you must\", \"you have to\", \"you should\", \"you better\", \"you need to\",\n",
    "\n",
    "        # Sarcastic / condescending tones\n",
    "        \"obviously\", \"clearly\", \"of course you would\", \"as if\", \"whatever\", \"yeah right\",\n",
    "        \"sure thing\", \"you think so\", \"what a joke\", \"get real\", \"unbelievable\", \"ridiculous\",\n",
    "\n",
    "        # Hostility / emotional aggression\n",
    "        \"hate\", \"disgusting\", \"gross\", \"awful\", \"terrible\", \"horrible\", \"nasty\",\n",
    "        \"idiotic\", \"moronic\", \"pathetic\", \"shame on you\", \"who cares\", \"no one cares\",\n",
    "        \"screw you\", \"go to hell\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def compute_politeness_from_lexicon(text, lexicon):\n",
    "    text_lower = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text_lower)\n",
    "\n",
    "    if not tokens:\n",
    "        return pd.Series({\n",
    "            \"polite_rate\": 0.0,\n",
    "            \"impolite_rate\": 0.0,\n",
    "            \"politeness_score\": 0.0\n",
    "        })\n",
    "\n",
    "    polite_words = {w for w in lexicon[\"polite_markers\"] if \" \" not in w}\n",
    "    polite_phrases = [w for w in lexicon[\"polite_markers\"] if \" \" in w]\n",
    "\n",
    "    impolite_words = {w for w in lexicon[\"impolite_markers\"] if \" \" not in w}\n",
    "    impolite_phrases = [w for w in lexicon[\"impolite_markers\"] if \" \" in w]\n",
    "\n",
    "    # single-word matches\n",
    "    polite_rate = sum(t in polite_words for t in tokens) / len(tokens)\n",
    "    impolite_rate = sum(t in impolite_words for t in tokens) / len(tokens)\n",
    "\n",
    "    # phrase matches\n",
    "    polite_rate += sum(p in text_lower for p in polite_phrases) / len(tokens)\n",
    "    impolite_rate += sum(p in text_lower for p in impolite_phrases) / len(tokens)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"polite_rate\": polite_rate,\n",
    "        \"impolite_rate\": impolite_rate,\n",
    "        \"politeness_score\": polite_rate - impolite_rate\n",
    "    })\n",
    "\n",
    "# Apply to dataframe\n",
    "politeness_features = df_bias[\"text\"].progress_apply(\n",
    "    compute_politeness_from_lexicon, lexicon=politeness_lexicon\n",
    ")\n",
    "\n",
    "# Add into dataframe\n",
    "df_bias[\"polite_rate\"] = politeness_features[\"polite_rate\"]\n",
    "df_bias[\"impolite_rate\"] = politeness_features[\"impolite_rate\"]\n",
    "df_bias[\"politeness_score\"] = politeness_features[\"politeness_score\"]\n",
    "\n",
    "# Check output\n",
    "stats_by_label_politeness = df_bias.groupby(\"label\")[\"politeness_score\"].describe()\n",
    "stats_by_label_politeness = stats_by_label_politeness[[\"count\", \"mean\", \"std\", \"max\"]]\n",
    "\n",
    "print(stats_by_label_politeness) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca32954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 6334/23707 [14:18<43:29,  6.66it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [47:50<00:00,  8.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average stance probabilities by label:\n",
      "       prob_left  prob_center  prob_right\n",
      "label                                    \n",
      "0       0.612678     0.258223    0.129099\n",
      "1       0.648738     0.204894    0.146231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Political leaning ---\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"launch/POLITICS\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"matous-volf/political-leaning-politics\")\n",
    "model.eval()\n",
    "\n",
    "# Prediction helper function\n",
    "def classify_political_leaning(text):\n",
    "    try:\n",
    "        tokens = tokenizer(text[:512], return_tensors=\"pt\")  # limit to first 512 tokens for speed\n",
    "        output = model(**tokens)\n",
    "        logits = output.logits\n",
    "        leaning_idx = argmax(logits, dim=1).item()\n",
    "        probs = softmax(logits, dim=1)\n",
    "        score = probs[0, leaning_idx].item()\n",
    "\n",
    "        label_map = {0: \"left\", 1: \"center\", 2: \"right\"}\n",
    "        leaning_label = label_map.get(leaning_idx, \"unknown\")\n",
    "\n",
    "        return pd.Series({\n",
    "            \"stance_pred\": leaning_label,\n",
    "            \"stance_score\": score,\n",
    "            \"prob_left\": probs[0, 0].item(),\n",
    "            \"prob_center\": probs[0, 1].item(),\n",
    "            \"prob_right\": probs[0, 2].item()\n",
    "        })\n",
    "\n",
    "    except Exception:\n",
    "        return pd.Series({\n",
    "            \"stance_pred\": \"error\",\n",
    "            \"stance_score\": 0.0,\n",
    "            \"prob_left\": 0.0,\n",
    "            \"prob_center\": 0.0,\n",
    "            \"prob_right\": 0.0\n",
    "        })\n",
    "\n",
    "# Apply to dataset\n",
    "stance_features = df_bias[\"text\"].progress_apply(classify_political_leaning)\n",
    "df_bias[\"stance_pred\"] = stance_features[\"stance_pred\"]\n",
    "df_bias[\"stance_score\"] = stance_features[\"stance_score\"]\n",
    "df_bias[\"prob_left\"] = stance_features[\"prob_left\"]\n",
    "df_bias[\"prob_center\"] = stance_features[\"prob_center\"]\n",
    "df_bias[\"prob_right\"] = stance_features[\"prob_right\"]\n",
    "\n",
    "# Check output of political stance leanings\n",
    "print(\"\\nAverage stance probabilities by label:\")\n",
    "print(df_bias.groupby(\"label\")[[\"prob_left\", \"prob_center\", \"prob_right\"]].mean()) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:17<00:00, 1332.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identity Term Frequency Comparison:\n",
      "\n",
      "=== gender_identity_rate ===\n",
      "         count      mean       std  min       25%       50%       75%  \\\n",
      "label                                                                   \n",
      "0       9101.0  0.046038  0.027518  0.0  0.025316  0.045817  0.064759   \n",
      "1      14606.0  0.032907  0.029982  0.0  0.004425  0.027192  0.054130   \n",
      "\n",
      "            max  \n",
      "label            \n",
      "0      0.153734  \n",
      "1      0.156383   \n",
      "\n",
      "=== race_ethnicity_identity_rate ===\n",
      "         count      mean       std  min  25%  50%       75%       max\n",
      "label                                                                \n",
      "0       9101.0  0.000970  0.002300  0.0  0.0  0.0  0.001418  0.047809\n",
      "1      14606.0  0.000637  0.002567  0.0  0.0  0.0  0.000000  0.061617 \n",
      "\n",
      "=== religion_identity_rate ===\n",
      "         count      mean       std  min  25%  50%  75%       max\n",
      "label                                                           \n",
      "0       9101.0  0.000123  0.000918  0.0  0.0  0.0  0.0  0.031936\n",
      "1      14606.0  0.000186  0.001573  0.0  0.0  0.0  0.0  0.041543 \n",
      "\n",
      "=== nationality_identity_rate ===\n",
      "         count      mean       std  min  25%  50%  75%      max\n",
      "label                                                          \n",
      "0       9101.0  0.000528  0.001618  0.0  0.0  0.0  0.0  0.03858\n",
      "1      14606.0  0.000597  0.002091  0.0  0.0  0.0  0.0  0.04000 \n",
      "\n",
      "=== orientation_identity_rate ===\n",
      "         count      mean       std  min  25%  50%  75%       max\n",
      "label                                                           \n",
      "0       9101.0  0.000108  0.000836  0.0  0.0  0.0  0.0  0.047486\n",
      "1      14606.0  0.000100  0.001995  0.0  0.0  0.0  0.0  0.085799 \n",
      "\n",
      "=== disability_identity_rate ===\n",
      "         count      mean       std  min  25%  50%  75%       max\n",
      "label                                                           \n",
      "0       9101.0  0.000133  0.000747  0.0  0.0  0.0  0.0  0.029210\n",
      "1      14606.0  0.000179  0.001212  0.0  0.0  0.0  0.0  0.037249 \n",
      "\n",
      "=== age_identity_rate ===\n",
      "         count      mean       std  min  25%       50%       75%       max\n",
      "label                                                                     \n",
      "0       9101.0  0.003165  0.003475  0.0  0.0  0.002304  0.004573  0.028065\n",
      "1      14606.0  0.001499  0.002664  0.0  0.0  0.000000  0.002188  0.042904 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Identity terms ---\n",
    "# Define lexicons\n",
    "identity_lexicons = {\n",
    "    \"gender\": {\n",
    "        \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"woman\", \"man\",\n",
    "        \"male\", \"female\", \"boy\", \"girl\", \"mother\", \"father\",\n",
    "        \"sister\", \"brother\"\n",
    "    },\n",
    "\n",
    "    \"race_ethnicity\": {\n",
    "        \"white\", \"black\", \"asian\", \"latino\", \"hispanic\",\n",
    "        \"african\", \"european\", \"indian\", \"arab\"\n",
    "    },\n",
    "\n",
    "    \"religion\": {\n",
    "        \"christian\", \"muslim\", \"jewish\", \"hindu\", \"buddhist\",\n",
    "        \"islam\", \"christianity\", \"judaism\"\n",
    "    },\n",
    "\n",
    "    \"nationality\": {\n",
    "        \"american\", \"british\", \"german\", \"french\", \"spanish\",\n",
    "        \"russian\", \"chinese\", \"japanese\", \"italian\", \"canadian\"\n",
    "    },\n",
    "\n",
    "    \"orientation\": {\n",
    "        \"gay\", \"lesbian\", \"bisexual\", \"transgender\", \"queer\", \"lgbt\"\n",
    "    },\n",
    "\n",
    "    \"disability\": {\n",
    "        \"blind\", \"deaf\", \"autistic\", \"disabled\", \"mental\",\n",
    "        \"handicapped\", \"wheelchair\"\n",
    "    },\n",
    "\n",
    "    \"age\": {\n",
    "        \"child\", \"kid\", \"youth\", \"teen\", \"adult\", \"elderly\",\n",
    "        \"senior\", \"old\", \"young\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compute rates\n",
    "def compute_identity_term_rates(text, lexicons=identity_lexicons):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    total = len(tokens)\n",
    "\n",
    "    if total == 0:\n",
    "        return pd.Series({f\"{k}_identity_rate\": 0.0 for k in lexicons})\n",
    "\n",
    "    features = {}\n",
    "    for category, words in lexicons.items():\n",
    "        count = sum(t in words for t in tokens)\n",
    "        features[f\"{category}_identity_rate\"] = count / total\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Apply to dataframe\n",
    "identity = df_bias[\"text\"].progress_apply(compute_identity_term_rates)\n",
    "\n",
    "for col in identity.columns:\n",
    "    df_bias[col] = identity[col]\n",
    "\n",
    "# Check output\n",
    "print(\"\\nIdentity Term Frequency Comparison:\\n\")\n",
    "\n",
    "for col in identity.columns:\n",
    "    print(f\"=== {col} ===\")\n",
    "    print(df_bias.groupby(\"label\")[col].describe(), \"\\n\") # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841de4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:08<00:00, 2763.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hedging / Modality Rate Comparison:\n",
      "         count      mean       std  min       25%       50%       75%  \\\n",
      "label                                                                   \n",
      "0       9101.0  0.014677  0.007716  0.0  0.009160  0.013636  0.019074   \n",
      "1      14606.0  0.009545  0.006938  0.0  0.004444  0.008299  0.013201   \n",
      "\n",
      "            max  \n",
      "label            \n",
      "0      0.068807  \n",
      "1      0.063325  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Hedging and modality ---\n",
    "# Define lexicons\n",
    "hedge_lexicon = {\n",
    "    \"modals\": [\n",
    "        \"may\", \"might\", \"could\", \"should\", \"would\",\n",
    "        \"can\", \"shall\", \"ought\",\n",
    "        \"possibly\", \"conceivably\"\n",
    "    ],\n",
    "\n",
    "    \"adverbs\": [\n",
    "        \"perhaps\", \"probably\", \"possibly\", \"apparently\", \"likely\",\n",
    "        \"evidently\", \"presumably\", \"seemingly\", \"maybe\",\n",
    "        \"hopefully\", \"arguably\", \"theoretically\",\n",
    "        \"hypothetically\", \"roughly\", \"approximately\",\n",
    "        \"almost\", \"virtually\", \"essentially\", \"generally\",\n",
    "        \"typically\", \"usually\", \"frequently\", \"often\",\n",
    "        \"mostly\", \"largely\", \"somewhat\", \"partially\"\n",
    "    ],\n",
    "\n",
    "    \"verbs\": [\n",
    "        \"seem\", \"appear\", \"suggest\", \"imply\", \"assume\", \"believe\",\n",
    "        \"think\", \"guess\", \"suspect\", \"estimate\", \"suppose\",\n",
    "        \"consider\", \"imagine\", \"indicate\", \"hint\", \"propose\",\n",
    "        \"speculate\", \"predict\", \"infer\", \"presume\",\n",
    "    ],\n",
    "\n",
    "    \"phrases\": [\n",
    "        \"it seems\", \"it appears\", \"it is possible\", \"it is likely\",\n",
    "        \"there is a chance\", \"it could be\", \"it might be\",\n",
    "        \"to some extent\", \"in a way\", \"in some cases\",\n",
    "        \"in certain respects\", \"it may be that\",\n",
    "        \"one could argue\", \"one might think\",\n",
    "        \"it could appear\", \"it could suggest\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Compute hedging frequency\n",
    "def compute_hedge_rate(text, lexicon):\n",
    "    text_lower = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text_lower)\n",
    "    total_tokens = len(tokens)\n",
    "    if total_tokens == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Single-word matches\n",
    "    single_words = set(lexicon[\"modals\"] + lexicon[\"adverbs\"] + lexicon[\"verbs\"])\n",
    "    count_single = sum(t in single_words for t in tokens)\n",
    "\n",
    "    # Multi-word phrase matches\n",
    "    count_phrases = sum(p in text_lower for p in lexicon[\"phrases\"])\n",
    "\n",
    "    return (count_single + count_phrases) / total_tokens\n",
    "\n",
    "# Apply to dataframe\n",
    "hedge_rate = df_bias[\"hedge_rate\"] = df_bias[\"text\"].progress_apply(lambda x: compute_hedge_rate(x, hedge_lexicon))\n",
    "\n",
    "# Check output\n",
    "stats_by_label = df_bias.groupby(\"label\")[\"hedge_rate\"].describe()\n",
    "print(\"\\nHedging / Modality Rate Comparison:\")\n",
    "print(stats_by_label) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d745d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:17<00:00, 1355.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical words Rate Comparison:\n",
      "         count      mean       std  min       25%       50%       75%  \\\n",
      "label                                                                   \n",
      "0       9101.0  0.014699  0.007259  0.0  0.009472  0.014006  0.019118   \n",
      "1      14606.0  0.008067  0.005673  0.0  0.004000  0.007174  0.011050   \n",
      "\n",
      "            max  \n",
      "label            \n",
      "0      0.059084  \n",
      "1      0.049505  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Categorical statements ---\n",
    "# Define lexicons\n",
    "categorical_lexicon = [\n",
    "    # Universal quantifiers\n",
    "    \"always\", \"never\", \"ever\", \"all\", \"none\",\n",
    "    \"every\", \"everyone\", \"everything\", \"everybody\",\n",
    "    \"nobody\", \"nothing\", \"noone\", \"no-one\",\n",
    "\n",
    "    # Absolutes / extremes\n",
    "    \"completely\", \"entirely\", \"absolutely\", \"totally\",\n",
    "    \"utterly\", \"purely\", \"perfectly\", \"fully\",\n",
    "    \"definitely\", \"certainly\", \"inevitably\", \"unquestionably\",\n",
    "    \"undeniably\", \"unconditionally\",\n",
    "\n",
    "    # Extremeness / maximality\n",
    "    \"forever\", \"eternal\", \"eternally\", \"infinite\", \"infinitely\",\n",
    "    \"permanent\", \"permanently\", \"final\", \"finally\",\n",
    "    \"ultimate\", \"ultimately\", \"guaranteed\", \"guarantee\",\n",
    "\n",
    "    # Strong modal certainty\n",
    "    \"must\", \"cannot\", \"can't\", \"will\", \"won't\",\n",
    "    \"always\", \"never\",\n",
    "\n",
    "    # Strong evaluative absolutes\n",
    "    \"best\", \"worst\", \"only\", \"everyone\", \"nobody\",\n",
    "    \"universal\", \"universally\", \"all-time\"\n",
    "]\n",
    "\n",
    "# Compute rate of categorical statements\n",
    "def compute_categorical_rate(text, lexicon=categorical_lexicon):\n",
    "    text_lower = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text_lower)\n",
    "    total_tokens = len(tokens)\n",
    "    if total_tokens == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(t in lexicon for t in tokens) / total_tokens\n",
    "\n",
    "# Apply to dataframe\n",
    "categorical_rate = df_bias[\"categorical_rate\"] = df_bias[\"text\"].progress_apply(compute_categorical_rate)\n",
    "\n",
    "# Check output\n",
    "stats_by_label = df_bias.groupby(\"label\")[\"categorical_rate\"].describe()\n",
    "print(\"\\nCategorical words Rate Comparison:\")\n",
    "print(stats_by_label) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a355441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:16<00:00, 1475.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assertive words Rate Comparison:\n",
      "         count      mean       std  min       25%       50%       75%  \\\n",
      "label                                                                   \n",
      "0       9101.0  0.004606  0.004173  0.0  0.001504  0.003683  0.006319   \n",
      "1      14606.0  0.002974  0.003760  0.0  0.000000  0.001939  0.004298   \n",
      "\n",
      "            max  \n",
      "label            \n",
      "0      0.038405  \n",
      "1      0.041958  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Assertiveness ---\n",
    "# Define lexicons\n",
    "assertive_lexicon = [\n",
    "    # Strong factual/claim verbs\n",
    "    \"prove\", \"proves\", \"proved\",\n",
    "    \"demonstrate\", \"demonstrates\", \"demonstrated\",\n",
    "    \"show\", \"shows\", \"shown\",\n",
    "    \"confirm\", \"confirms\", \"confirmed\",\n",
    "    \"establish\", \"establishes\", \"established\",\n",
    "    \"verify\", \"verifies\", \"verified\",\n",
    "\n",
    "    # Strong certainty adverbs\n",
    "    \"clearly\", \"certainly\", \"definitely\", \"undeniably\",\n",
    "    \"obviously\", \"evidently\", \"unquestionably\",\n",
    "    \"undoubtedly\", \"incontrovertibly\", \"irrefutably\",\n",
    "    \"surely\", \"decisively\",\n",
    "\n",
    "    # Strong necessity / obligation signals\n",
    "    \"must\", \"cannot\", \"can't\", \"will\", \"won't\",\n",
    "    \"have to\", \"has to\", \"need to\",\n",
    "\n",
    "    # Strong evaluative certainty\n",
    "    \"without a doubt\", \"beyond doubt\", \"beyond question\",\n",
    "    \"it is clear\", \"it is certain\",\n",
    "    \"everyone knows\", \"it is obvious\",\n",
    "\n",
    "    # Rhetorical emphasis\n",
    "    \"in fact\", \"the truth is\", \"the reality is\"\n",
    "]\n",
    "\n",
    "# Compute rate of assertive words\n",
    "def compute_assertive_rate(text, lexicon=assertive_lexicon):\n",
    "    text_lower = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text_lower)\n",
    "    total_tokens = len(tokens)\n",
    "    if total_tokens == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(t in lexicon for t in tokens) / total_tokens\n",
    "\n",
    "# Apply to dataframe\n",
    "assertive_rate = df_bias[\"assertive_rate\"]   = df_bias[\"text\"].progress_apply(compute_assertive_rate)\n",
    "\n",
    "# Check output\n",
    "stats_by_label = df_bias.groupby(\"label\")[\"assertive_rate\"].describe()\n",
    "print(\"\\nAssertive words Rate Comparison:\")\n",
    "print(stats_by_label) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:14<00:00, 1599.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subjective words Rate Comparison:\n",
      "         count      mean       std  min       25%       50%       75%  \\\n",
      "label                                                                   \n",
      "0       9101.0  0.005252  0.004020  0.0  0.002418  0.004478  0.007386   \n",
      "1      14606.0  0.003241  0.003598  0.0  0.000000  0.002342  0.004745   \n",
      "\n",
      "            max  \n",
      "label            \n",
      "0      0.033141  \n",
      "1      0.037528  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Subjectivity ---\n",
    "# Define lexicons\n",
    "subjective_lexicon = [\n",
    "    # subjective adjectives\n",
    "    \"good\", \"bad\", \"terrible\", \"wonderful\", \"awful\", \"amazing\",\n",
    "    \"horrible\", \"excellent\", \"disgusting\", \"fantastic\", \"stupid\",\n",
    "    \"brilliant\", \"ridiculous\", \"shocking\", \"tragic\", \"beautiful\",\n",
    "    \"ugly\", \"unfair\", \"unjust\", \"biased\", \"corrupt\",\n",
    "\n",
    "    # subjective verbs (opinions, evaluations)\n",
    "    \"believe\", \"think\", \"feel\", \"guess\", \"suspect\", \"argue\",\n",
    "    \"claim\", \"insist\", \"support\", \"oppose\", \"criticize\",\n",
    "    \"praise\", \"endorse\", \"blame\", \"doubt\", \"prefer\", \"hate\",\n",
    "    \"love\", \"enjoy\", \"fear\", \"worry\"\n",
    "]\n",
    "\n",
    "# Compute rate of subjective words\n",
    "def compute_subjectivity_score(text, lexicon=subjective_lexicon):\n",
    "    text_lower = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text_lower)\n",
    "    total = len(tokens)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return sum(t in lexicon for t in tokens) / total\n",
    "\n",
    "# Apply to dataframe\n",
    "df_bias[\"subjectivity_score\"] = df_bias[\"text\"].progress_apply(compute_subjectivity_score)\n",
    "\n",
    "# Check output\n",
    "stats_by_label = df_bias.groupby(\"label\")[\"subjectivity_score\"].describe()\n",
    "print(\"\\nSubjective words Rate Comparison:\")\n",
    "print(stats_by_label) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:13<00:00, 1732.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pos_rate_count  pos_rate_mean  pos_rate_std  pos_rate_min  \\\n",
      "label                                                              \n",
      "0              9101.0       0.033994      0.012791           0.0   \n",
      "1             14606.0       0.043895      0.018656           0.0   \n",
      "\n",
      "       pos_rate_25%  pos_rate_50%  pos_rate_75%  pos_rate_max  neg_rate_count  \\\n",
      "label                                                                           \n",
      "0          0.024969      0.033105      0.041729      0.106814          9101.0   \n",
      "1          0.030588      0.042105      0.055307      0.144000         14606.0   \n",
      "\n",
      "       neg_rate_mean  ...  neg_rate_75%  neg_rate_max  polarity_score_count  \\\n",
      "label                 ...                                                     \n",
      "0           0.030889  ...      0.038690      0.099190                9101.0   \n",
      "1           0.041190  ...      0.053977      0.161392               14606.0   \n",
      "\n",
      "       polarity_score_mean  polarity_score_std  polarity_score_min  \\\n",
      "label                                                                \n",
      "0                 0.003105            0.019209           -0.083700   \n",
      "1                 0.002705            0.032198           -0.136076   \n",
      "\n",
      "       polarity_score_25%  polarity_score_50%  polarity_score_75%  \\\n",
      "label                                                               \n",
      "0               -0.008982            0.004045            0.015588   \n",
      "1               -0.019063            0.002398            0.023644   \n",
      "\n",
      "       polarity_score_max  \n",
      "label                      \n",
      "0                0.086556  \n",
      "1                0.144000  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Positivity and negativity --\n",
    "# Load lexicons\n",
    "\n",
    "def load_lexicon(path):\n",
    "    \"\"\"Loads a lexicon file (one word per line, allows comment lines).\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return {line.strip().lower() for line in f\n",
    "                if line.strip() and not line.startswith(\";\")}\n",
    "\n",
    "positive_lexicon = load_lexicon(#Insert path to positive-words-dictionary.txt here (can be downloaded from github repository))\n",
    "negative_lexicon = load_lexicon(#Insert path to negative-words-dictionary.txt here (can be downloaded from github repository))\n",
    "\n",
    "# Compute rates\n",
    "def compute_emotional_tone_from_lexicons(text, pos_lex, neg_lex):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    if not tokens:\n",
    "        return pd.Series({\"pos_rate\": 0.0, \"neg_rate\": 0.0, \"polarity_score\": 0.0})\n",
    "    pos_rate = sum(t in pos_lex for t in tokens) / len(tokens)\n",
    "    neg_rate = sum(t in neg_lex for t in tokens) / len(tokens)\n",
    "    return pd.Series({\n",
    "        \"pos_rate\": pos_rate,\n",
    "        \"neg_rate\": neg_rate,\n",
    "        \"polarity_score\": pos_rate - neg_rate\n",
    "    })\n",
    "\n",
    "# Apply to dataframe\n",
    "emotional_tone = df_bias[\"text\"].progress_apply(compute_emotional_tone_from_lexicons, pos_lex=positive_lexicon, neg_lex=negative_lexicon)\n",
    "\n",
    "# Add into df\n",
    "df_bias[\"pos_rate\"] = emotional_tone[\"pos_rate\"]\n",
    "df_bias[\"neg_rate\"] = emotional_tone[\"neg_rate\"]\n",
    "df_bias[\"polarity_score\"] = emotional_tone[\"polarity_score\"]\n",
    "\n",
    "# Check output\n",
    "stats_by_label_emotional_tone = df_bias.groupby(\"label\")[[\"pos_rate\", \"neg_rate\", \"polarity_score\"]].describe()\n",
    "\n",
    "# Flatten hierarchical columns\n",
    "stats_by_label_emotional_tone.columns = ['_'.join(col).strip() for col in stats_by_label_emotional_tone.columns.values]\n",
    "print(stats_by_label_emotional_tone) # 0=Human, 1=AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2892259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23707/23707 [00:12<00:00, 1892.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We/They Feature Comparison:\n",
      "\n",
      "=== we_pronoun_rate ===\n",
      "         count      mean       std  min  25%       50%       75%       max\n",
      "label                                                                     \n",
      "0       9101.0  0.004938  0.007160  0.0  0.0  0.002725  0.006466  0.113456\n",
      "1      14606.0  0.005450  0.009087  0.0  0.0  0.001560  0.007159  0.080675 \n",
      "\n",
      "=== they_pronoun_rate ===\n",
      "         count      mean       std  min       25%       50%       75%  \\\n",
      "label                                                                   \n",
      "0       9101.0  0.008490  0.007725  0.0  0.003040  0.006460  0.011561   \n",
      "1      14606.0  0.008355  0.008644  0.0  0.002568  0.005985  0.011282   \n",
      "\n",
      "            max  \n",
      "label            \n",
      "0      0.109677  \n",
      "1      0.091190   \n",
      "\n",
      "=== we_they_ratio ===\n",
      "         count      mean       std  min  25%       50%  75%   max\n",
      "label                                                            \n",
      "0       9101.0  0.998576  2.128660  0.0  0.0  0.333333  1.0  48.0\n",
      "1      14606.0  1.040896  2.447203  0.0  0.0  0.166667  1.0  37.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- We vs. They (dropped because unstable) ---\n",
    "# Define lexicons\n",
    "we_pronouns = {\"we\", \"us\", \"our\", \"ours\", \"ourselves\"}\n",
    "they_pronouns = {\"they\", \"them\", \"their\", \"theirs\", \"themselves\"}\n",
    "\n",
    "# Compute rate of pronouns\n",
    "def compute_we_they_features(text):\n",
    "    text_lower = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text_lower)\n",
    "    total = len(tokens)\n",
    "    if total == 0:\n",
    "        return pd.Series({\n",
    "            \"we_pronoun_rate\": 0.0,\n",
    "            \"they_pronoun_rate\": 0.0,\n",
    "            \"we_they_ratio\": 0.0\n",
    "        })\n",
    "\n",
    "    we_count = sum(t in we_pronouns for t in tokens)\n",
    "    they_count = sum(t in they_pronouns for t in tokens)\n",
    "\n",
    "    we_rate = we_count / total\n",
    "    they_rate = they_count / total\n",
    "\n",
    "    # avoid division by zero\n",
    "    if they_count == 0:\n",
    "        ratio = float(we_count > 0)  # 1 if we>0, else 0\n",
    "    else:\n",
    "        ratio = we_count / they_count\n",
    "\n",
    "    return pd.Series({\n",
    "        \"we_pronoun_rate\": we_rate,\n",
    "        \"they_pronoun_rate\": they_rate,\n",
    "        \"we_they_ratio\": ratio\n",
    "    })\n",
    "\n",
    "# Apply to dataframe\n",
    "we_they = df_bias[\"text\"].progress_apply(compute_we_they_features)\n",
    "df_bias[\"we_pronoun_rate\"] = we_they[\"we_pronoun_rate\"]\n",
    "df_bias[\"they_pronoun_rate\"] = we_they[\"they_pronoun_rate\"]\n",
    "df_bias[\"we_they_ratio\"] = we_they[\"we_they_ratio\"]\n",
    "\n",
    "# Check output\n",
    "print(\"\\nWe/They Feature Comparison:\\n\")\n",
    "\n",
    "for col in [\"we_pronoun_rate\", \"they_pronoun_rate\", \"we_they_ratio\"]:\n",
    "    print(f\"=== {col} ===\")\n",
    "    print(df_bias.groupby(\"label\")[col].describe(), \"\\n\") # 0=Human, 1=AI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
